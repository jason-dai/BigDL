# BigDL-LLM

`bigdl-llm` is a library for running **LLM** on Intel **GPU** (including *integrated graphics or iGPU*) and **CPU** with very low latency[^1] (for any **PyTorch** model).

[^1]: Performance varies by use, configuration and other factors. `bigdl-llm` may not optimize to the same degree for non-Intel products. Learn more at www.Intel.com/PerformanceIndex.

## Run LLM models using `bigdl-llm`
### Chat on local PC (w/ iGPU)
Running Text-Generation-WebUI

### Run in Docker
Running built-in examples and benchmarks

## Write PyTorch code using `bigdl-llm`
### Install
1. Windows & GPU
2. Linux & GPU
3. Windows & CPU
4. Linux & CPU

### Inference
1. INT4
2. FP16/BF16 (Speculative)
3. FP8/INT8
 
### Finetuning

## Use other libraries with `bigdl-llm`
1. HF
2. ModelScope
3. ...
